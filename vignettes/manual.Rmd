---
title: "How to use (and build on) mevolCVP2"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{manual}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Preamble

This tiny package (finally) releases the [seminal idea coined](https://www.sciencedirect.com/science/article/abs/pii/S030544031200355X) by Allowen Evin and colleagues in _Journal of Archaeological Science_ in 2013.

This approach was previously available as a single script (circa 2013), then wrapped into a [package](https://github.com/vbonhomme/mevolCVP) (circa 2018) without any change to the code and not released on CRAN. Here, we rethink and rewrote the latter code to have something easier to build upon.

The Evin et al. (2013) paper, among others, used "predictive" discriminant analyses in morphometrical analyses, particularly in the context of landmark-based morphometrics which uses principal component analyses prior to LDA.

The idea was two-fold: i) parameterized the optimal number of principal components to retain for further linear discriminant analyses, and ii) evaluated the effects of unbalanced sample sized between groups for LDA.

Yet this approach deeply changed the way we see, parameterize and do LDAs in morphometrics, it actualy boils down, statistically, to the following two core ideas.

# Rationale

The rational of permutationnal and parameterized discriminant analyses boils down to two ideas:

1) **The number of variables is discriminant analyses matters** and can be optimized.

This is typically the case for landmark approaches where a "preliminary" PCA mandatory and its components later used for LDA. But even for outline-based approaches, where quantitative variables are not orthogonal and independant by construction, their number also matters.

In other words, the common view that a linear discriminant analysis accuracy increases monotonously with the number of variables you feed it with is wrong. 

Of course, we let apart here many other fundamental aspects such as measurement error, it is neither reasonable nor meaningful (nor even allowed) to use 200 principal components or 60 harmonics to train models that will ultimately be used on degraded archaeological material (or even fresh)

2) **The group sample sizes matters**. In spite of prior used by default in LDA (eg in `MASS::lda`), the class proportions is not enough to unbias class predictions and consequently global accuracy. In other words, if you have a dataset with 90% of class A and 10% of class B, you will "mechanically" have a better cross-validation accuracy for class A and likely an optimistic global accuracy too. This is definitely not what we want when we do LDA, particularly when we used trained models to then make predictions, for example in the archaeological context.

We realized both i) and ii) empirically and yet we are confident that these can probably be explained mathematically we never find out how and why.

Along the last ~10 years (2014->2024), we thus systematically investigated the number of components (or harmonics) to retain and used permutations to obtain balanced datasets.

As a side effect, the use of permutationnal approaches typically allows to obtain the true distribution of $H_0$, in other words to compare model performances against pure random. For morphometrical studies, this allowed to detect subtle yet significant morphological signals even with relatively low accuracies. In other words, even a model with 60% accuracy (which is far from impressive) may be better than random (ie a binomial or a coin) and thus reveal a true morphological signal.

# Architecture
Programmatically, the last section boils down to: i) retain more or less variables, ii) do a lot of permutations. Consequently, this packages consists of very few and thin wrappers around (very) common statistical operations. We nevertheless hope it will set a common ground for morphometrics enthusiasts, help ourselves and newcomers in daily explorations and ease more sophisticated ones. 

Here, all analyses will start with a `data.frame` with:
1) the first column being the grouping structure that can be turned into a factor (eg a factor or character)
2) at least one other column being a (or many more) quantitative variable.

We first load the package using `library`. We also specify a random seed which is a good idea to have the best of both worlds: randomness _and_ replicability across your sessions.
```{r}
library(mevolCVP2)
set.seed(2329)
```

We will use the built-in `pig` dataset, borrowed from the _JAS_ paper. Let's have a look to the first and last 5 rows and the first 3 columns:
```{r pig-pres}
minipig <- pig[c(1:5, 166:171), 1:6]
minipig
```

Then three core helpers ease the common operations:

```{r core-helpers}
randomize(minipig)  # look at the first column
balance(minipig, 2) # count rows in each group
retain(minipig, 3)  # count quantitative variables columns
```

The workhorse of this package is a thin wrapper on top of `MASS::lda` that directly returns the confusion matrix. `MASS::lda` is passed with `CV=TRUE` which returns leave-one-out cross-validated predictions, digested into a confusion matrix. Let's try it on the full `pig` dataset:

```{r lda1}
cm <- lda1(pig)
```

We can calculate accuracy indices on this confusion matrix. We will later see how to obtain additional metrics from the latter.

```{r metrics}
acc(cm)
acc_classes(cm)
acc_all(cm)
```

`pig` is "highly" unbalanced in terms of sample sizes between the two groups. We expect a naive LDA on this dataset to be biased and an LDA on balanced dataset to be more realistic in terms of class accuracies.

```{r}
pig %>% lda1() %>% acc_classes()
pig %>% balance() %>% lda1() %>% acc_classes()
```

Above, we realize that the class accuracy of the `WB` group (larger by more than a factor 3) is drastically reduced and actually performs less well than `DP`.

Whether these values are better than pure random have to be properly tested. We expect, in such binary case, something around 50/50 for pure random (ie a binomial) but we can be away from this ideal case that hypothesize a gentle structure of variables used by the LDA (which is not the case).

```{r}
pig %>% balance() %>% randomize() %>% lda1() %>% acc_classes()
```

We already have pretty much everything we need!

One more helpful helper is `partition` that eases what we have done just before and returns a list with four different datasets with self-explanatory names:

```{r}
pigs1 <- partition(pig)
pigs1
```

Now, we gonna take profit of functionnal programming tools provided by [purrr](https://purrr.tidyverse.org/). If you are not familiar with this package, stop everything and go have a look to it.

```{r}
library(purrr)
pigs1 %>% map(\(.df1) .df1 %>% lda1() %>% acc_all())
```

The results above should replicate the expectations and results we observed before, doing it "by hand". 

We can arrange them a bit and put them in a nice tibble. We just add a couple more tidying functions to the call:

```{r}
library(dplyr)
pigs1 %>% map(\(.df1) .df1 %>% lda1() %>% acc_all()) %>% 
  do.call("rbind", .) %>% as_tibble(rownames = "partition")
```
Can you see where we're going here? So let fun begins and we will replicate the four LDAs done just before using one more layer of `map_dfr` which is just a `map` that returns its results into a "rbinded" `data.frame`. Let's try with 10 iterations.


```{r}
K=10
do1 <- function() {pigs1 %>% map(\(.df1) .df1 %>% lda1() %>% acc_all()) %>% 
  do.call("rbind", .) %>% as_tibble(rownames = "partition")}
pigs10 <- map_dfr(1:10, \(i) do1())
pigs10
```

Let's digest this a bit with `dplyr`
```{r}
pigs10 %>% 
  group_by(partition) %>% 
  summarize(across(everything(), median))

```

